{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(size, length):\n",
    "    data = torch.rand(size, length).round()\n",
    "    target = torch.fmod(torch.sum(data, 1), 2).squeeze()\n",
    "    target = target.long()\n",
    "\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, length, num_units, num_layers, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.m0 = nn.Linear(length, num_units)\n",
    "        self.h0 = nn.Linear(length, num_units)\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # create the forget, input, candidate, output linearities for each layer\n",
    "\n",
    "        self.forget_lin = [nn.Linear(num_units, num_units).cuda() for i in range(num_layers)]\n",
    "        self.input_lin = [nn.Linear(num_units, num_units).cuda() for i in range(num_layers)]\n",
    "        self.candidate_lin = [nn.Linear(num_units, num_units).cuda() for i in range(num_layers)]\n",
    "        self.output_lin = [nn.Linear(num_units, num_units).cuda() for i in range(num_layers)]\n",
    "\n",
    "        self.activation = nn.Linear(num_units + num_units, num_classes)\n",
    "\n",
    "    def step(self, h, m, forget_lin_, input_lin_, candidate_lin_, output_lin_):\n",
    "        # create the gates for each linearites, using the sigmoid as the activation function\n",
    "        forget_gate = F.sigmoid(forget_lin_(h))\n",
    "        input_gate = F.sigmoid(input_lin_(h))\n",
    "        candidate = F.tanh(candidate_lin_(h))\n",
    "        output_gate = F.sigmoid(output_lin_(h))\n",
    "\n",
    "        # memory vector m, and a hidden vector h\n",
    "        m = (input_gate * candidate) + (forget_gate * m)\n",
    "        h = F.tanh(output_gate * m)\n",
    "\n",
    "        return h, m\n",
    "\n",
    "    def forward(self, inputs, hidden=None, force=True, steps=0):\n",
    "        if force or steps == 0: steps = len(inputs)\n",
    "        outputs = Variable(torch.zeros(steps, 1, 1))\n",
    "\n",
    "        m = self.m0(inputs)\n",
    "        h = self.h0(inputs)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # for each layer, compute the vectors h, m, which are the inputs fed into the following layer\n",
    "            forget_lin_ = self.forget_lin[i]\n",
    "            input_lin_ = self.input_lin[i]\n",
    "            candidate_lin_ = self.candidate_lin[i]\n",
    "            output_lin_ = self.output_lin[i]\n",
    "            h, m = self.step(h, m, forget_lin_, input_lin_, candidate_lin_, output_lin_)\n",
    "\n",
    "        outputs = F.log_softmax(self.activation(torch.cat((h, m), 1)), dim=1)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.5\n",
    "NUM_UNITS = 1\n",
    "NUM_CLASSES = 2\n",
    "NUM_LAYERS = 1\n",
    "LENGTH = 30\n",
    "\n",
    "model = Net(length=LENGTH, num_units=NUM_UNITS, num_layers=NUM_LAYERS, num_classes=NUM_CLASSES)\n",
    "model.cuda()\n",
    "opt = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, log_interval):\n",
    "    TOTAL_SIZE = 100000\n",
    "    BATCH_SIZE = 10000\n",
    "    NUM_BATCHES = TOTAL_SIZE // BATCH_SIZE\n",
    "    \n",
    "    d, t = generate_data(size=TOTAL_SIZE, length=LENGTH)\n",
    "    print(\"shape of data:\", d.shape)\n",
    "    print(\"hidden units:\", NUM_UNITS, \", layers:\", NUM_LAYERS, \", sequence length:\", LENGTH)\n",
    "    idx = torch.randperm(TOTAL_SIZE)\n",
    "    \n",
    "    print(\"Running on train data\")\n",
    "    for epoch in range(1, EPOCH + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        idx = torch.randperm(TOTAL_SIZE)\n",
    "        for i in range(NUM_BATCHES):\n",
    "            batch_idx = idx[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
    "            data, target = d[batch_idx], t[batch_idx]\n",
    "\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss = loss.data.item()\n",
    "            \n",
    "        if epoch % log_interval == 0:\n",
    "            print('Train Epoch: {} \\tLoss: {:.6f}'.format(epoch, epoch_loss))\n",
    "    print('Final Epoch: {} \\tLoss: {:.6f}'.format(epoch, epoch_loss))\n",
    "\n",
    "\n",
    "def test():\n",
    "    print(\"Running on test data\")\n",
    "    TEST_SIZE = 5000\n",
    "    BATCH_SIZE = 100\n",
    "    NUM_BATCHES = TEST_SIZE // BATCH_SIZE\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data, target = generate_data(size = TEST_SIZE, length=LENGTH)\n",
    "    batches = int(TEST_SIZE / BATCH_SIZE)\n",
    "\n",
    "    for i in range(batches):\n",
    "        d = data[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
    "        t = target[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
    "        d, t = d.cuda(), t.cuda()\n",
    "        d, t = Variable(d, volatile=True), Variable(t, volatile=True)\n",
    "        out = model(d)\n",
    "        test_loss += F.nll_loss(out, t).data.item()\n",
    "        pred = out.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(t.data.view_as(pred)).cpu().sum().item()\n",
    "    print(\"Test loss: \", test_loss / batches)\n",
    "    print(\"Number correct: \", correct / TEST_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NUM_UNITS=100, LENGTH=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data: torch.Size([100000, 15])\n",
      "hidden units: 100 layers 1 sequence length: 15\n",
      "Running on train data\n",
      "Train Epoch: 100 \tLoss: 0.693226\n",
      "Train Epoch: 200 \tLoss: 0.691268\n",
      "Train Epoch: 300 \tLoss: 0.041477\n",
      "Train Epoch: 400 \tLoss: 0.013026\n",
      "Train Epoch: 500 \tLoss: 0.005603\n",
      "Train Epoch: 600 \tLoss: 0.002506\n",
      "Train Epoch: 700 \tLoss: 0.002005\n",
      "Train Epoch: 800 \tLoss: 0.000566\n",
      "Train Epoch: 900 \tLoss: 0.000219\n",
      "Train Epoch: 1000 \tLoss: 0.000282\n",
      "Final Epoch: 1000 \tLoss: 0.000282\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.005\n",
    "NUM_UNITS = 100\n",
    "NUM_CLASSES = 2\n",
    "NUM_LAYERS = 1\n",
    "LENGTH = 15\n",
    "EPOCH = 1000\n",
    "LOG_EVERY_EPOCH = 100\n",
    "\n",
    "model = Net(length=LENGTH, num_units=NUM_UNITS, num_layers=NUM_LAYERS, num_classes=NUM_CLASSES)\n",
    "model.cuda()\n",
    "opt = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train(EPOCH, LOG_EVERY_EPOCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUM_UNITS=100, LENGTH=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data: torch.Size([100000, 20])\n",
      "hidden units: 100 , layers: 1 , sequence length: 20\n",
      "Running on train data\n",
      "Train Epoch: 100 \tLoss: 0.691040\n",
      "Train Epoch: 200 \tLoss: 0.687722\n",
      "Train Epoch: 300 \tLoss: 0.680695\n",
      "Train Epoch: 400 \tLoss: 0.671677\n",
      "Train Epoch: 500 \tLoss: 0.427290\n",
      "Train Epoch: 600 \tLoss: 0.147723\n",
      "Train Epoch: 700 \tLoss: 0.050909\n",
      "Train Epoch: 800 \tLoss: 0.024202\n",
      "Train Epoch: 900 \tLoss: 0.010777\n",
      "Train Epoch: 1000 \tLoss: 0.006203\n",
      "Train Epoch: 1100 \tLoss: 0.004822\n",
      "Train Epoch: 1200 \tLoss: 0.002946\n",
      "Train Epoch: 1300 \tLoss: 0.001825\n",
      "Train Epoch: 1400 \tLoss: 0.000892\n",
      "Train Epoch: 1500 \tLoss: 0.000643\n",
      "Train Epoch: 1600 \tLoss: 0.000374\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-8247c169224b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLOG_EVERY_EPOCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-151-dc4c19b3a599>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, log_interval)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.005\n",
    "NUM_UNITS = 100\n",
    "NUM_CLASSES = 2\n",
    "NUM_LAYERS = 1\n",
    "LENGTH = 20\n",
    "EPOCH = 1000\n",
    "LOG_EVERY_EPOCH = 100\n",
    "\n",
    "model = Net(length=LENGTH, num_units=NUM_UNITS, num_layers=NUM_LAYERS, num_classes=NUM_CLASSES)\n",
    "model.cuda()\n",
    "opt = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train(EPOCH, LOG_EVERY_EPOCH)\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUM_UNITS=500, LENGTH=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data: torch.Size([100000, 25])\n",
      "hidden units: 500 , layers: 1 , sequence length: 25\n",
      "Running on train data\n",
      "Train Epoch: 100 \tLoss: 0.690813\n",
      "Train Epoch: 200 \tLoss: 0.684318\n",
      "Train Epoch: 300 \tLoss: 0.668866\n",
      "Train Epoch: 400 \tLoss: 0.637986\n",
      "Train Epoch: 500 \tLoss: 0.610248\n",
      "Train Epoch: 600 \tLoss: 0.570222\n",
      "Train Epoch: 700 \tLoss: 0.550168\n",
      "Train Epoch: 800 \tLoss: 0.508000\n",
      "Train Epoch: 900 \tLoss: 0.495314\n",
      "Train Epoch: 1000 \tLoss: 0.469450\n",
      "Train Epoch: 1100 \tLoss: 0.458772\n",
      "Train Epoch: 1200 \tLoss: 0.442051\n",
      "Train Epoch: 1300 \tLoss: 0.422570\n",
      "Train Epoch: 1400 \tLoss: 0.414792\n",
      "Train Epoch: 1500 \tLoss: 0.406297\n",
      "Train Epoch: 1600 \tLoss: 0.388922\n",
      "Train Epoch: 1700 \tLoss: 0.376663\n",
      "Train Epoch: 1800 \tLoss: 0.373813\n",
      "Train Epoch: 1900 \tLoss: 0.375763\n",
      "Train Epoch: 2000 \tLoss: 0.357228\n",
      "Train Epoch: 2100 \tLoss: 0.368847\n",
      "Train Epoch: 2200 \tLoss: 0.360956\n",
      "Train Epoch: 2300 \tLoss: 0.338571\n",
      "Train Epoch: 2400 \tLoss: 0.344475\n",
      "Train Epoch: 2500 \tLoss: 0.333713\n",
      "Train Epoch: 2600 \tLoss: 0.320933\n",
      "Train Epoch: 2700 \tLoss: 0.309373\n",
      "Train Epoch: 2800 \tLoss: 0.321624\n",
      "Train Epoch: 2900 \tLoss: 0.318749\n",
      "Train Epoch: 3000 \tLoss: 0.314235\n",
      "Train Epoch: 3100 \tLoss: 0.311194\n",
      "Train Epoch: 3200 \tLoss: 0.308943\n",
      "Train Epoch: 3300 \tLoss: 0.308574\n",
      "Train Epoch: 3400 \tLoss: 0.300514\n",
      "Train Epoch: 3500 \tLoss: 0.306070\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.005\n",
    "NUM_UNITS = 500\n",
    "NUM_CLASSES = 2\n",
    "NUM_LAYERS = 1\n",
    "LENGTH = 25\n",
    "EPOCH = 5000\n",
    "LOG_EVERY_EPOCH = 100\n",
    "\n",
    "model = Net(length=LENGTH, num_units=NUM_UNITS, num_layers=NUM_LAYERS, num_classes=NUM_CLASSES)\n",
    "model.cuda()\n",
    "opt = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "train(EPOCH, LOG_EVERY_EPOCH)\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
